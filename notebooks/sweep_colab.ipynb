{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Hyperparameter Sweep — Entropy Selection Project\n",
                "\n",
                "Runs `scripts/run_sweep.py` on an A100 GPU. Results are saved to Google Drive.\n",
                "\n",
                "**Workflow:**\n",
                "1. Install dependencies\n",
                "2. Clone / pull repo from GitHub\n",
                "3. Mount Google Drive (results saved there, survives session restarts)\n",
                "4. Configure and dry-run the sweep\n",
                "5. Run the sweep (use `--skip-existing` to resume across sessions)\n",
                "6. Select best hyperparams → final evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ── 1. Install dependencies ──────────────────────────────────────────\n",
                "print('Installing PyTorch 2.5.1 with CUDA 12.4...')\n",
                "!pip install -q torch==2.5.1 --index-url https://download.pytorch.org/whl/cu124\n",
                "\n",
                "print('Updating system libraries...')\n",
                "!sudo add-apt-repository ppa:ubuntu-toolchain-r/test -y > /dev/null 2>&1\n",
                "!sudo apt-get update > /dev/null 2>&1\n",
                "!sudo apt-get install --only-upgrade libstdc++6 -y > /dev/null 2>&1\n",
                "\n",
                "print('Installing PyTorch Geometric...')\n",
                "!pip install -q torch-geometric\n",
                "!pip install -q torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-2.5.0+cu124.html\n",
                "!pip install -q filelock\n",
                "\n",
                "import torch, torch_geometric\n",
                "print(f'\\n✓ PyTorch: {torch.__version__}')\n",
                "print(f'✓ PyG: {torch_geometric.__version__}')\n",
                "print(f'✓ CUDA: {torch.cuda.is_available()}')\n",
                "if torch.cuda.is_available():\n",
                "    print(f'✓ GPU: {torch.cuda.get_device_name(0)}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ── 2. Clone / pull repo ─────────────────────────────────────────────\n",
                "import os, sys\n",
                "\n",
                "GITHUB_REPO = 'econci474/GDL'\n",
                "CLONE_DIR   = 'entropy-selection'\n",
                "\n",
                "if os.path.exists(f'{CLONE_DIR}/src'):\n",
                "    print('Repo already present — pulling latest...')\n",
                "    !git -C {CLONE_DIR} pull\n",
                "else:\n",
                "    # Add GITHUB_TOKEN to Colab Secrets (left panel > key icon) for private repos\n",
                "    try:\n",
                "        from google.colab import userdata\n",
                "        token = userdata.get('GITHUB_TOKEN')\n",
                "        clone_url = f'https://{token}@github.com/{GITHUB_REPO}.git'\n",
                "        print('Cloning with token...')\n",
                "    except Exception:\n",
                "        clone_url = f'https://github.com/{GITHUB_REPO}.git'\n",
                "        print('Cloning public...')\n",
                "    !git clone {clone_url} {CLONE_DIR}\n",
                "\n",
                "os.chdir(CLONE_DIR)\n",
                "sys.path.insert(0, os.getcwd())\n",
                "print(f'\\n✓ Working directory: {os.getcwd()}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ── 3. Mount Google Drive and redirect results ───────────────────────\n",
                "# Results are written here after EVERY run, so they survive session restarts.\n",
                "from google.colab import drive\n",
                "drive.mount('/content/drive')\n",
                "\n",
                "import os\n",
                "DRIVE_RESULTS = '/content/drive/MyDrive/GDL/sweep_results'\n",
                "os.makedirs(DRIVE_RESULTS, exist_ok=True)\n",
                "\n",
                "# Symlink results/ -> Drive\n",
                "if not os.path.islink('results'):\n",
                "    if os.path.exists('results'):\n",
                "        !rm -rf results\n",
                "    !ln -s {DRIVE_RESULTS} results\n",
                "    print(f'✓ results/ -> {DRIVE_RESULTS}')\n",
                "else:\n",
                "    print(f'✓ results/ already linked to {os.readlink(\"results\")}')\n",
                "\n",
                "for d in ['results/runs', 'results/classifier_heads', 'results/tables', 'results/figures']:\n",
                "    os.makedirs(d, exist_ok=True)\n",
                "print('✓ Drive mounted and results directory ready')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ── 4. Sweep configuration ───────────────────────────────────────────\n",
                "# Estimated runtime: ~35 hrs on A100 with these settings.\n",
                "# Use --skip-existing to resume safely across sessions.\n",
                "\n",
                "DATASETS   = 'Cora PubMed Roman-empire Squirrel'\n",
                "MODELS     = 'GCN'\n",
                "LOSS_TYPES = 'ce_only ce_plus_R'\n",
                "K_VALUES   = 'all'          # K=1..8\n",
                "SEEDS      = '0 1 2'\n",
                "SPLIT_MODE = 'first'        # split 0 only for heterophilous (faster)\n",
                "\n",
                "print('Sweep configuration:')\n",
                "print(f'  Datasets:   {DATASETS}')\n",
                "print(f'  Models:     {MODELS}')\n",
                "print(f'  Loss types: {LOSS_TYPES}')\n",
                "print(f'  K values:   {K_VALUES}')\n",
                "print(f'  Seeds:      {SEEDS}')\n",
                "print(f'  Split mode: {SPLIT_MODE}')\n",
                "print()\n",
                "print('Estimated total runs: ~12,480')\n",
                "print('Estimated runtime:    ~35 hrs on A100')\n",
                "print('Use --skip-existing to resume if session expires.')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ── 5. Dry run — preview commands ────────────────────────────────────\n",
                "import subprocess, sys\n",
                "\n",
                "def build_sweep_cmd(dry_run=True, skip_existing=False):\n",
                "    cmd = (\n",
                "        [sys.executable, 'scripts/run_sweep.py']\n",
                "        + ['--datasets']   + DATASETS.split()\n",
                "        + ['--models']     + MODELS.split()\n",
                "        + ['--loss-types'] + LOSS_TYPES.split()\n",
                "        + ['--K-values']   + K_VALUES.split()\n",
                "        + ['--seeds']      + SEEDS.split()\n",
                "        + ['--split-mode', SPLIT_MODE]\n",
                "    )\n",
                "    if dry_run:\n",
                "        cmd.append('--dry-run')\n",
                "    if skip_existing:\n",
                "        cmd.append('--skip-existing')\n",
                "    return cmd\n",
                "\n",
                "dry_cmd = build_sweep_cmd(dry_run=True)\n",
                "print('Command:', ' '.join(dry_cmd))\n",
                "print()\n",
                "subprocess.run(dry_cmd)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ── 6. Run the sweep ─────────────────────────────────────────────────\n",
                "# ⚠️  Verify the dry run output above before running this cell!\n",
                "#\n",
                "# RESUMING after a session restart:\n",
                "#   - Re-run cells 1-3 to reinstall deps, pull latest code, remount Drive\n",
                "#   - Then run this cell — --skip-existing will skip already-completed runs\n",
                "\n",
                "import subprocess, sys, time\n",
                "\n",
                "cmd = build_sweep_cmd(dry_run=False, skip_existing=True)\n",
                "print('Starting sweep...')\n",
                "print('Command:', ' '.join(cmd))\n",
                "print()\n",
                "\n",
                "t0 = time.time()\n",
                "result = subprocess.run(cmd)\n",
                "elapsed = (time.time() - t0) / 60\n",
                "\n",
                "print(f'\\n✓ Sweep complete in {elapsed:.1f} min')\n",
                "print(f'Exit code: {result.returncode}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ── 7. Check sweep progress ──────────────────────────────────────────\n",
                "# Run this any time to see how many runs have completed.\n",
                "import pandas as pd\n",
                "from pathlib import Path\n",
                "\n",
                "sweep_csv = Path('results/sweep_results.csv')\n",
                "if sweep_csv.exists():\n",
                "    df = pd.read_csv(sweep_csv)\n",
                "    print(f'Completed runs: {len(df)}')\n",
                "    print(f'Datasets:  {sorted(df[\"dataset\"].unique().tolist())}')\n",
                "    print(f'Models:    {sorted(df[\"model\"].unique().tolist())}')\n",
                "    print(f'Loss types:{sorted(df[\"loss_type\"].unique().tolist())}')\n",
                "    print(f'\\nRuns per (dataset, model, loss_type):')\n",
                "    print(df.groupby(['dataset','model','loss_type']).size().to_string())\n",
                "else:\n",
                "    print('No sweep_results.csv yet — sweep has not started.')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ── 8. Select best hyperparameters ───────────────────────────────────\n",
                "# Run AFTER the sweep is complete.\n",
                "import subprocess, sys, pandas as pd\n",
                "from pathlib import Path\n",
                "\n",
                "result = subprocess.run(\n",
                "    [sys.executable, 'src/select_hyperparams.py', '--hetero-split-mode', 'first'],\n",
                "    capture_output=False\n",
                ")\n",
                "print(f'Exit code: {result.returncode}')\n",
                "\n",
                "best_csv = Path('results/best_hyperparams.csv')\n",
                "if best_csv.exists():\n",
                "    best = pd.read_csv(best_csv)\n",
                "    print(f'\\nBest hyperparams ({len(best)} configs):')\n",
                "    print(best[['dataset','model','loss_type','lr','weight_decay','hidden_dim','total_val_loss']].to_string(index=False))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ── 9. Final test evaluation ─────────────────────────────────────────\n",
                "# ⚠️  Run ONCE after selecting best hyperparams.\n",
                "# This touches the test set — do not use for hyperparameter decisions!\n",
                "import subprocess, sys, pandas as pd\n",
                "from pathlib import Path\n",
                "\n",
                "result = subprocess.run(\n",
                "    [sys.executable, 'src/evaluate_final.py',\n",
                "     '--from-best-hyperparams',\n",
                "     '--seeds', '0', '1', '2',\n",
                "     '--K-values', 'all',\n",
                "     '--split-mode', 'first'],\n",
                "    capture_output=False\n",
                ")\n",
                "print(f'Exit code: {result.returncode}')\n",
                "\n",
                "final_csv = Path('results/tables/final_results.csv')\n",
                "if final_csv.exists():\n",
                "    final = pd.read_csv(final_csv)\n",
                "    print(f'\\nFinal results ({len(final)} rows):')\n",
                "    print(final[['dataset','model','loss_type','K','seed','test_acc']]\n",
                "          .sort_values(['dataset','model','loss_type','K'])\n",
                "          .to_string(index=False))"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "A100",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
