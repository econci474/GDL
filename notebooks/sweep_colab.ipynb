{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Hyperparameter Sweep — Entropy Selection Project\n",
                "\n",
                "Runs `scripts/run_sweep.py` on an A100 GPU. Results are saved to Google Drive.\n",
                "\n",
                "**Workflow:**\n",
                "1. Install dependencies\n",
                "2. Clone repo from GitHub\n",
                "3. Mount Google Drive (results saved there)\n",
                "4. Configure and run the sweep\n",
                "5. (Optional) Select best hyperparams and run final evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ── 1. Install dependencies ──────────────────────────────────────────\n",
                "print('Installing PyTorch 2.5.1 with CUDA 12.4...')\n",
                "!pip install -q torch==2.5.1 --index-url https://download.pytorch.org/whl/cu124\n",
                "\n",
                "print('Updating system libraries...')\n",
                "!sudo add-apt-repository ppa:ubuntu-toolchain-r/test -y > /dev/null 2>&1\n",
                "!sudo apt-get update > /dev/null 2>&1\n",
                "!sudo apt-get install --only-upgrade libstdc++6 -y > /dev/null 2>&1\n",
                "\n",
                "print('Installing PyTorch Geometric...')\n",
                "!pip install -q torch-geometric\n",
                "!pip install -q torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-2.5.0+cu124.html\n",
                "!pip install -q filelock\n",
                "\n",
                "import torch\n",
                "import torch_geometric\n",
                "print(f'\\n✓ PyTorch: {torch.__version__}')\n",
                "print(f'✓ PyG: {torch_geometric.__version__}')\n",
                "print(f'✓ CUDA: {torch.cuda.is_available()}')\n",
                "if torch.cuda.is_available():\n",
                "    print(f'✓ GPU: {torch.cuda.get_device_name(0)}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ── 2. Clone repo ────────────────────────────────────────────────────\n",
                "import os, sys\n",
                "\n",
                "GITHUB_REPO = 'econci474/GDL'  # <-- your repo\n",
                "CLONE_DIR   = 'entropy-selection'\n",
                "\n",
                "if os.path.exists(f'{CLONE_DIR}/src'):\n",
                "    print('Repo already cloned — pulling latest...')\n",
                "    !git -C {CLONE_DIR} pull\n",
                "else:\n",
                "    # Use Colab Secrets: Secrets > Add > Name: GITHUB_TOKEN, Value: your PAT\n",
                "    try:\n",
                "        from google.colab import userdata\n",
                "        token = userdata.get('GITHUB_TOKEN')\n",
                "        clone_url = f'https://{token}@github.com/{GITHUB_REPO}.git'\n",
                "        print('Cloning with token...')\n",
                "    except Exception:\n",
                "        clone_url = f'https://github.com/{GITHUB_REPO}.git'\n",
                "        print('Cloning public...')\n",
                "    !git clone {clone_url} {CLONE_DIR}\n",
                "\n",
                "os.chdir(CLONE_DIR)\n",
                "sys.path.insert(0, os.getcwd())\n",
                "print(f'\\n✓ Working directory: {os.getcwd()}')\n",
                "!ls -1"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ── 3. Mount Google Drive and redirect results ───────────────────────\n",
                "from google.colab import drive\n",
                "drive.mount('/content/drive')\n",
                "\n",
                "import os\n",
                "DRIVE_RESULTS = '/content/drive/MyDrive/GDL/sweep_results'\n",
                "os.makedirs(DRIVE_RESULTS, exist_ok=True)\n",
                "\n",
                "# Symlink results/ -> Drive so checkpoints and CSVs persist\n",
                "if not os.path.islink('results'):\n",
                "    if os.path.exists('results'):\n",
                "        !cp -r results {DRIVE_RESULTS}/results_backup 2>/dev/null || true\n",
                "        !rm -rf results\n",
                "    !ln -s {DRIVE_RESULTS} results\n",
                "    print(f'✓ results/ -> {DRIVE_RESULTS}')\n",
                "else:\n",
                "    print(f'✓ results/ already symlinked to {os.readlink(\"results\")}')\n",
                "\n",
                "# Create required subdirectories\n",
                "for d in ['results/runs', 'results/classifier_heads', 'results/tables', 'results/figures']:\n",
                "    os.makedirs(d, exist_ok=True)\n",
                "print('✓ Drive mounted and results directory ready')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ── 4. Sweep configuration ───────────────────────────────────────────\n",
                "# Edit these to control what gets swept.\n",
                "# Use 'all' to sweep everything defined in config.py.\n",
                "\n",
                "DATASETS   = 'all'          # or e.g. 'Cora PubMed'\n",
                "MODELS     = 'GCN'          # or 'all' for GCN GAT GraphSAGE\n",
                "LOSS_TYPES = 'all'          # or e.g. 'ce_only weighted_ce'\n",
                "K_VALUES   = 'all'          # or e.g. '2 4 6 8'\n",
                "SEEDS      = '0 1'          # or 'all' for seeds 0-3\n",
                "SPLIT_MODE = 'first'        # 'first' = split 0 only for hetero (faster)\n",
                "\n",
                "# Dry run first to see what will be run\n",
                "DRY_RUN = True\n",
                "\n",
                "print('Sweep configuration:')\n",
                "print(f'  Datasets:   {DATASETS}')\n",
                "print(f'  Models:     {MODELS}')\n",
                "print(f'  Loss types: {LOSS_TYPES}')\n",
                "print(f'  K values:   {K_VALUES}')\n",
                "print(f'  Seeds:      {SEEDS}')\n",
                "print(f'  Split mode: {SPLIT_MODE}')\n",
                "print(f'  Dry run:    {DRY_RUN}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ── 5. Dry run — preview commands ────────────────────────────────────\n",
                "import subprocess, sys\n",
                "\n",
                "def build_sweep_cmd(dry_run=True):\n",
                "    cmd = [\n",
                "        sys.executable, 'scripts/run_sweep.py',\n",
                "        '--datasets'] + DATASETS.split() + [\n",
                "        '--models']     + MODELS.split() + [\n",
                "        '--loss-types'] + LOSS_TYPES.split() + [\n",
                "        '--K-values']   + K_VALUES.split() + [\n",
                "        '--seeds']      + SEEDS.split() + [\n",
                "        '--split-mode', SPLIT_MODE,\n",
                "    ]\n",
                "    if dry_run:\n",
                "        cmd.append('--dry-run')\n",
                "    return cmd\n",
                "\n",
                "dry_cmd = build_sweep_cmd(dry_run=True)\n",
                "print('Command:', ' '.join(dry_cmd))\n",
                "print()\n",
                "result = subprocess.run(dry_cmd, capture_output=False)\n",
                "print(f'\\nDry run exit code: {result.returncode}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ── 6. Run the sweep ─────────────────────────────────────────────────\n",
                "# ⚠️  Only run this after verifying the dry run output above!\n",
                "# This will take a long time. Results are saved to Drive as each run completes.\n",
                "\n",
                "import subprocess, sys, time\n",
                "\n",
                "cmd = build_sweep_cmd(dry_run=False)\n",
                "print('Starting sweep...')\n",
                "print('Command:', ' '.join(cmd))\n",
                "print()\n",
                "\n",
                "t0 = time.time()\n",
                "result = subprocess.run(cmd)\n",
                "elapsed = (time.time() - t0) / 60\n",
                "\n",
                "print(f'\\n✓ Sweep complete in {elapsed:.1f} min')\n",
                "print(f'Exit code: {result.returncode}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ── 7. Check sweep results ───────────────────────────────────────────\n",
                "import pandas as pd\n",
                "from pathlib import Path\n",
                "\n",
                "sweep_csv = Path('results/sweep_results.csv')\n",
                "if sweep_csv.exists():\n",
                "    df = pd.read_csv(sweep_csv)\n",
                "    print(f'Sweep results: {len(df)} rows')\n",
                "    print(f'\\nDatasets covered: {df[\"dataset\"].unique().tolist()}')\n",
                "    print(f'Models covered:   {df[\"model\"].unique().tolist()}')\n",
                "    print(f'Loss types:       {df[\"loss_type\"].unique().tolist()}')\n",
                "    print(f'\\nBest val losses per (dataset, model, loss_type):')\n",
                "    print(df.groupby(['dataset','model','loss_type'])['best_val_loss'].min().to_string())\n",
                "else:\n",
                "    print('No sweep_results.csv yet — run the sweep first.')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ── 8. Select best hyperparameters ───────────────────────────────────\n",
                "# Run after the sweep to pick the best config per (dataset, model, loss_type)\n",
                "\n",
                "result = subprocess.run(\n",
                "    [sys.executable, 'src/select_hyperparams.py', '--hetero-split-mode', 'first'],\n",
                "    capture_output=False\n",
                ")\n",
                "print(f'\\nExit code: {result.returncode}')\n",
                "\n",
                "best_csv = Path('results/best_hyperparams.csv')\n",
                "if best_csv.exists():\n",
                "    best = pd.read_csv(best_csv)\n",
                "    print(f'\\nBest hyperparams ({len(best)} configs):')\n",
                "    print(best[['dataset','model','loss_type','lr','weight_decay','hidden_dim','total_val_loss']].to_string(index=False))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ── 9. Final test evaluation (run ONCE after hyperparameter selection) ─\n",
                "# ⚠️  Only run this AFTER selecting best hyperparams.\n",
                "# This evaluates on the test set — do not use for hyperparameter decisions!\n",
                "\n",
                "result = subprocess.run(\n",
                "    [sys.executable, 'src/evaluate_final.py',\n",
                "     '--from-best-hyperparams',\n",
                "     '--seeds', 'all',\n",
                "     '--K-values', 'all',\n",
                "     '--split-mode', 'first'],\n",
                "    capture_output=False\n",
                ")\n",
                "print(f'\\nExit code: {result.returncode}')\n",
                "\n",
                "final_csv = Path('results/tables/final_results.csv')\n",
                "if final_csv.exists():\n",
                "    final = pd.read_csv(final_csv)\n",
                "    print(f'\\nFinal results ({len(final)} rows):')\n",
                "    print(final[['dataset','model','loss_type','K','seed','test_acc']]\n",
                "          .sort_values(['dataset','model','loss_type','K'])\n",
                "          .to_string(index=False))"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "A100",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}